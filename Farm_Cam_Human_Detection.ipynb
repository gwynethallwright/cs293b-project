{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/f0/cfe88d262c67825b20d396c778beca21829da061717c7aaa8b421ae5132e/opencv_python-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (28.2MB)\n",
      "\u001b[K     |████████████████████████████████| 28.2MB 23.8MB/s eta 0:00:01   |██████████                      | 8.9MB 5.3MB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /root/miniconda3/lib/python3.7/site-packages (from opencv-python) (1.17.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.2.0.34\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PaHQ-5hLctj1"
   },
   "outputs": [],
   "source": [
    "def check_create_dirs(outer_dir, inner_dir):\n",
    "    if not os.path.exists(outer_dir):\n",
    "        os.makedirs(outer_dir)\n",
    "    if not os.path.exists(outer_dir + \"/\" + inner_dir):\n",
    "        os.makedirs(outer_dir + \"/\" + inner_dir)\n",
    "\n",
    "def generate_filepath(writefile_prefix, directory, frame_number):\n",
    "    return directory + \"/\" + writefile_prefix[:-4] + \"/\" + writefile_prefix[:-4] + \"_%d.png\" % frame_number\n",
    "\n",
    "def clear_extracted(directory_1=\"./contains_human_extracted\", directory_2=\"./human_less_extracted\"):\n",
    "    for directory in [directory_1, directory_2]:\n",
    "        if os.path.exists(directory):\n",
    "            shutil.rmtree(directory)\n",
    "\n",
    "def extract_frames(readfile, writefile_prefix, directory, num_frames_to_save=15, resize=True, x_dim=50, y_dim=50):\n",
    "    video = cv2.VideoCapture(readfile)\n",
    "    num_frames = video.get(cv2.CAP_PROP_FRAME_COUNT) # Get number of frames in video\n",
    "    if num_frames < num_frames_to_save: # Reject if not enough frames\n",
    "      print(\"File \\\"\" + readfile + \"\\\" has only \" + str(num_frames) + \" frames. Discarding.\")\n",
    "      return\n",
    "    factor = num_frames//num_frames_to_save\n",
    "    success, image = video.read()\n",
    "    frame_number = 0\n",
    "    num_frames_saved = 0\n",
    "    check_create_dirs(directory, writefile_prefix[:-4])\n",
    "    while success:\n",
    "      if (frame_number%factor) == 0:\n",
    "        # Resize and write image\n",
    "        if resize:\n",
    "          image = cv2.resize(image, (x_dim, y_dim))\n",
    "        cv2.imwrite(generate_filepath(writefile_prefix, directory, frame_number), image)\n",
    "        num_frames_saved += 1\n",
    "        # Check if we have saved enough frames\n",
    "        if num_frames_saved == num_frames_to_save:\n",
    "          return\n",
    "      # Get next frame\n",
    "      success, image = video.read()\n",
    "      frame_number += 1\n",
    "    return\n",
    "\n",
    "def extract_frames_all(read_directory_name, write_directory_name, num_frames_to_save, x_dim, y_dim):\n",
    "  for filename in os.listdir(read_directory_name):\n",
    "    extract_frames(read_directory_name + \"/\" + filename, filename, write_directory_name, num_frames_to_save, True, x_dim, y_dim)\n",
    "\n",
    "def read_training_data(base_directory_name, training_set, labels, x_dim, y_dim, contains_humans, num_sets_to_read):\n",
    "  num_sets_read = 0\n",
    "  for directory in os.listdir(base_directory_name):\n",
    "    frames_of_video = []\n",
    "    for filename in os.listdir(base_directory_name + \"/\" + directory):\n",
    "      train_image = image.load_img(base_directory_name + \"/\" + directory + \"/\" + filename, target_size=(x_dim, y_dim, 3))\n",
    "      train_image = image.img_to_array(train_image)\n",
    "      train_image = train_image/255\n",
    "      train_image = train_image.flatten()\n",
    "      frames_of_video.append(train_image)\n",
    "    if contains_humans:\n",
    "      labels.append([1,0])\n",
    "    else:\n",
    "      labels.append([0,1])\n",
    "    training_set.append(frames_of_video)\n",
    "    num_sets_read += 1\n",
    "    if num_sets_to_read:\n",
    "      if num_sets_to_read == num_sets_read:\n",
    "        print(str(num_sets_read) + \" videos have been read.\")\n",
    "        return\n",
    "  print(str(num_sets_read) + \" videos have been read.\")\n",
    "  return\n",
    "\n",
    "def get_data_and_labels(x_dim, y_dim, num_sets_to_read, directory_1=\"contains_human_extracted\", directory_2=\"human_less_extracted\"):\n",
    "  training_set = []\n",
    "  labels = []\n",
    "  read_training_data(directory_1, training_set, labels, x_dim, y_dim, True, num_sets_to_read)\n",
    "  read_training_data(directory_2, training_set, labels, x_dim, y_dim, False, num_sets_to_read)\n",
    "  full_data_set = np.array(training_set)\n",
    "  # This is where we split into training and testing sets.\n",
    "  (x_train, x_test, y_train, y_test) = train_test_split(full_data_set, labels, test_size=0.25, stratify=labels, random_state=42)\n",
    "  x_train = np.array(x_train)\n",
    "  y_train = np.array(y_train)\n",
    "  x_test = np.array(x_test)\n",
    "  y_test = np.array(y_test)\n",
    "  return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "def get_model(num_frames_to_save, x_dim, y_dim): # Change for better model.\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(100, input_shape=(num_frames_to_save, x_dim*y_dim*3)))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(100, activation='relu'))\n",
    "  model.add(Dense(2, activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 50\n",
    "y_dim = 50\n",
    "num_frames = 10 # How many frames per video should we extract?\n",
    "num_videos = 5 # How many videos should be use? None = use all available videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyHPB4nqXraW"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/notebooks/ALL_NOTEBOOKS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage from previous rounds successfully removed.\n",
      "File \"contains_human/FARM1-44218-2020_04_06__10_51_58.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"contains_human/FARM1-44221-2020_04_06__11_16_31.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"contains_human/FARM1-44236-2020_04_06__11_41_08.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"contains_human/FARM1-44208-2020_04_06__10_11_58.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"contains_human/FARM1-44232-2020_04_06__11_36_27.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"contains_human/FARM1-45850-2020_04_10__11_16_41.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"contains_human/FARM1-44206-2020_04_06__10_06_31.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"contains_human/FARM1-45859-2020_04_10__11_46_19.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"contains_human/FARM1-45842-2020_04_10__10_51_31.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"human_less/FARM1-45874-2020_04_10__15_01_14.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"human_less/FARM1-42738-2020_04_03__14_16_08.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"human_less/FARM1-42742-2020_04_03__14_36_10.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "File \"human_less/FARM1-44238-2020_04_06__13_06_16.mkv\" has only -1.3835058055282163e+17 frames. Discarding.\n",
      "Video frames successfully extracted.\n"
     ]
    }
   ],
   "source": [
    "# May not want to run if already extracted.\n",
    "clear_extracted()\n",
    "print(\"Garbage from previous rounds successfully removed.\")\n",
    "extract_frames_all(\"contains_human\", \"contains_human_extracted\", num_frames, x_dim, y_dim)\n",
    "extract_frames_all(\"human_less\", \"human_less_extracted\", num_frames, x_dim, y_dim)\n",
    "print(\"Video frames successfully extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 videos have been read.\n",
      "5 videos have been read.\n",
      "Train and test data read.\n"
     ]
    }
   ],
   "source": [
    "# Reading in the frames and splitting into test and training sets.\n",
    "(x_train, x_test, y_train, y_test) = get_data_and_labels(x_dim, y_dim, num_videos)\n",
    "print(\"Train and test data read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdvuJefuXsLV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model generated.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve model.\n",
    "lstm_model = get_model(num_frames, x_dim, y_dim)\n",
    "print(\"Model generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBclqNThOoPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 samples, validate on 3 samples\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 4s 502ms/step - loss: 1.5379 - acc: 0.5714 - val_loss: 1.3394 - val_acc: 0.3333\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 1.0722 - acc: 0.5714 - val_loss: 1.0559 - val_acc: 0.3333\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 1.1580 - acc: 0.4286 - val_loss: 0.8393 - val_acc: 0.3333\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.6252 - acc: 0.7143 - val_loss: 0.7003 - val_acc: 0.3333\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.6908 - acc: 0.7143 - val_loss: 0.6635 - val_acc: 0.6667\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Training and accuracy.\n",
    "lstm_model.fit(x_train, y_train, epochs=5, batch_size=2, validation_data=(x_test, y_test))\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 605,
     "status": "error",
     "timestamp": 1588391249593,
     "user": {
      "displayName": "Gwyneth Allwright",
      "photoUrl": "",
      "userId": "01192258260904707215"
     },
     "user_tz": 420
    },
    "id": "2UmUmBaFCtNN",
    "outputId": "b9daff63-c8cc-4443-cc6d-21189147da3b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ie9etM1DdvJL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Farm_Cam_Human_Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
